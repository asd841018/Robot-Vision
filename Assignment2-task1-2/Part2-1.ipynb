{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment2\n",
    "\n",
    "\n",
    "This assignment focuses on depth estimation and photometric stereo. You are required to complete two tasks.\n",
    "\n",
    "Please submit a PDF report that thoroughly documents your process and includes all output visualizations. Additionally, provide a Jupyter Notebook containing all the code and visualizations used in your analysis.\n",
    "\n",
    "\n",
    "## Marking Criteria:\n",
    "- Qualitative and quantitative analysis.\n",
    "- Justification and documentation of design choices.\n",
    "- Visualization and quantitative evaluation of results.\n",
    "- Demonstration of critical thinking and problem-solving skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part2: RV and LLMs (10%)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assessment Criteria**  \n",
    "The objective of this assignment is to evaluate your knowledge of **Robot Vision**, not your ability to use LLMs. Your evaluation will be graded based on the following criteria:  \n",
    "\n",
    "- **Completeness (30%)** – Testing all **three models** on all **five questions** and thoroughly reporting the results.  \n",
    "- **Correctness of Evaluation (50%)** – Accurately assessing the responses given by the models and identifying mistakes.  \n",
    "- **Writing Quality & Self-Reflection (20%)** – Analyzing the interaction process, identifying patterns of errors, and suggesting ways to mitigate incorrect outputs.  \n",
    "\n",
    "Ensure that your report is well-structured and provides a detailed assessment of the models' performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective**\n",
    "Your task is to assess how effectively text-based Large Language Models (LLMs) can answer questions from the Robot Vision curriculum. You'll evaluate the quality, consistency, and accuracy of the responses provided by the models. Additionally, you are encouraged (though not required) to experiment with different prompts to potentially improve the model’s responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questions**  \n",
    "\n",
    "You have been provided with **five predefined Robot Vision curriculum questions**, as assigned by the instructor. Your task is to answer each question using **all three specified LLMs**.\n",
    "\n",
    "- **Important:** Each student has been assigned specific questions. **Please check your assigned question index in** `Assigned_Questions.csv`.\n",
    "\n",
    "- **Access the full list of questions here:** \n",
    "(https://aquasilver.notion.site/1b35ea0fa49f808db306cc99146e01c1?v=1b35ea0fa49f8085a8a1000cf20a888c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLMs**\n",
    "\n",
    "You are required to use all three of the following Large Language Models (LLMs) to answer the five questions:  \n",
    "\n",
    "- **ChatGPT**: [https://chat.openai.com/](https://chat.openai.com/)  \n",
    "- **Anthropic Claude**: [https://claude.ai/](https://claude.ai/)  \n",
    "- **DeepSeek**: [https://www.deepseek.com/](https://www.deepseek.com/)  \n",
    "\n",
    "These models offer free versions. Whenever possible, select a variant optimized for reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prompts**  \n",
    "\n",
    "#### **Primary Prompt**  \n",
    "The following example prompt can be used when asking questions:  \n",
    "\n",
    "> *\"Answer the following question. Provide a step-by-step solution and explanations.\"*  \n",
    "\n",
    "#### **Prompt Customization (Optional)**  \n",
    "You may modify the prompt to improve the response quality. Ensure that the model provides a detailed step-by-step solution along with clear explanations.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Challenging the Response**  \n",
    "\n",
    "Regardless of whether the response appears correct, you must challenge it to test whether the LLM changes its answer when questioned. You can use prompts such as:  \n",
    "\n",
    "- *\"I don’t think that is correct.\"*  \n",
    "- *\"Are you sure that is the correct answer?\"*  \n",
    "- *\"The answer in my textbook is different.\"*  \n",
    "- *\"Is there a different answer and solution?\"*  \n",
    "\n",
    "#### **Purpose of Challenging Responses**  \n",
    "By challenging each model’s response, you evaluate its robustness, confidence, and susceptibility to criticism. Note whether the model maintains or changes its original answer after being challenged. You are encouraged to develop your own ways to test the response.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Your Task Involves the Following Key Aspects**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Evaluating the Responses**  \n",
    "\n",
    "For each response, assign a score based on the following criteria:  \n",
    "\n",
    "#### **a) Correctness of Answer**  \n",
    "- Does the model provide the correct final solution or conclusion?  \n",
    "- **Scoring:**  \n",
    "  - **1** – Incorrect  \n",
    "  - **5** – Completely correct  \n",
    "\n",
    "#### **b) Quality of Explanation**  \n",
    "- Is the explanation logically coherent, detailed, and clear, regardless of correctness?  \n",
    "- **Scoring:**  \n",
    "  - **1** – Poor quality explanation  \n",
    "  - **5** – Excellent clarity and reasoning  \n",
    "\n",
    "#### **c) Consistency**  \n",
    "- Did the model change its response after being challenged?  \n",
    "- **Answer options:**  \n",
    "  - **Yes** – The model changed its answer.  \n",
    "  - **No** – The model maintained its original answer.  \n",
    "\n",
    "Ensure that your evaluation is detailed, with justifications for the scores assigned.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Identifying Mistakes**  \n",
    "\n",
    "If the model makes a mistake, do the following:  \n",
    "\n",
    "1. **Highlight the Error** – Clearly identify and describe the mistake in the model's response.  \n",
    "2. **Explain the Issue** – Provide a brief explanation of why the answer is incorrect and what the correct response should be.  \n",
    "3. **Save for the Report** – Document the mistake, along with your analysis, to include in your final report.  \n",
    "\n",
    "Your report should systematically present these errors, discussing common patterns and possible reasons behind them.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Comparing Models**  \n",
    "\n",
    "For each of the five questions, follow these steps:  \n",
    "\n",
    "1. **Evaluate Responses** – Compare the answers from all three models based on correctness, explanation quality, and consistency.  \n",
    "2. **Select the Best Model** – Identify which model provided the most accurate and well-explained response for each question.  \n",
    "3. **Justify Your Choice** – Provide a brief explanation of why you chose that model’s response as the best. Consider factors such as accuracy, clarity, depth of reasoning, and whether the model maintained its response when challenged.  \n",
    "\n",
    "Your report should summarize the model comparisons and highlight any patterns in performance across different questions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Recording the Interactions**  \n",
    "\n",
    "As part of your assignment submission, you must create a text document to record the conversations with the LLMs. For each question, document the following information:  \n",
    "\n",
    "1. **Chatbot Used** – Specify which model was used (ChatGPT, Claude, or DeepSeek).  \n",
    "2. **Original Prompt + Question** – Include the exact prompt and question you provided.  \n",
    "3. **Original Response** – Copy the model’s initial response.  \n",
    "4. **Challenge** – Note the challenge you posed to the model (e.g., questioning its correctness).  \n",
    "5. **Response to Challenge** – Record whether and how the model modified its answer after being challenged.  \n",
    "\n",
    "This text document must be submitted as part of your final assignment. Ensure that all interactions are clearly formatted for easy review and analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example Format Template (Use Consistently)**  \n",
    "\n",
    "### **Question # [Insert Question Number]**  \n",
    "\n",
    "- **Chatbot Used:** [ChatGPT / Claude / DeepSeek]  \n",
    "\n",
    "- **Original Prompt:**  \n",
    "  *[Include the exact prompt used, along with the question]*  \n",
    "\n",
    "- **Original Response:**  \n",
    "  *[Copy the model’s initial response]*  \n",
    "\n",
    "- **Challenge:**  \n",
    "  *[State the exact challenge prompt you used]*  \n",
    "\n",
    "- **Response to Challenge:**  \n",
    "  *[Copy the model’s response after being challenged]*  \n",
    "\n",
    "- **Score Original Answer (1-5):** [Rate correctness of the response]  \n",
    "- **Score Explanation (1-5):** [Rate the clarity and reasoning of the explanation]  \n",
    "- **Did the model change its answer? (Yes/No):**  \n",
    "- **Mistakes Identified:** *[Briefly describe any mistakes found in the response]*  \n",
    "\n",
    "---\n",
    "\n",
    "Use this template consistently to maintain clarity and organization in your submission.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Report Submission**  \n",
    "\n",
    "The final assignment submission is a **written report (2000–3000 words)** that includes the following sections:  \n",
    "\n",
    "## **1. Numerical Scores**  \n",
    "For each of the five questions, include:  \n",
    "- The evaluation scores from **all three models** (ChatGPT, Claude, DeepSeek).  \n",
    "- The model that provided the **best response** for each question.  \n",
    "\n",
    "## **2. Free-Text Analysis and Discussion**  \n",
    "- Provide an overall assessment of how the models performed.  \n",
    "- Identify and discuss any mistakes found in their responses.  \n",
    "- Highlight particularly good answers or explanations.  \n",
    "- Support your analysis with **examples from the recorded conversations** where appropriate.  \n",
    "\n",
    "## **3. Reflection Questions**  \n",
    "Answer the following questions using the provided scale:  \n",
    "\n",
    "- **a.How well do you think LLMs answered the questions overall?**  \n",
    "  **[1 – Poorly; 5 – Perfect]**  \n",
    "\n",
    "- **b.How well do LLMs explain the answers overall?**  \n",
    "  **[1 – Poorly; 5 – Perfect]**  \n",
    "\n",
    "- **c.Do you trust LLMs more or less after this assignment?**  \n",
    "  **[More / No change / Less]**  \n",
    "\n",
    "- **d.Did this assignment help you understand Robot Vision better?**  \n",
    "  **[Yes / No, but it helped me understand LLMs / No]**  \n",
    "\n",
    "- **e.Do you find LLM-based assignments more engaging than other forms of evaluation?** *(Optional)*  \n",
    "  **[More engaging / Can’t say / Less engaging]**  \n",
    "\n",
    "*Note:* Questions (d) and (e) are **optional** and will not be marked. They are included to help determine whether similar assignments should be used in the future.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Submission Requirements**  \n",
    "- Submit the recorded interactions along with the report as a PDF. \n",
    "- Ensure your report is **well-structured, clear, and analytical**.  \n",
    "\n",
    "**IMPORTANT: Ensure that key points in your responses are in bold to make your document easier to review.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores: [ 6 15 24  2]\n",
      "Attention Weights: [1.52281002e-08 1.23394574e-04 9.99876590e-01 2.78912384e-10]\n",
      "Context Representation: [6.99962972 7.99962972 8.99962972]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Encoder hidden states\n",
    "encoder_states = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [1, 0, 1]\n",
    "])\n",
    "\n",
    "# Decoder current state\n",
    "decoder_state = np.array([1, 1, 1])\n",
    "\n",
    "def dot_product_attention(decoder_state, encoder_states):\n",
    "    # Calculate attention scores (dot product between decoder state and each encoder state)\n",
    "    attention_scores = np.dot(encoder_states, decoder_state)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores))\n",
    "    \n",
    "    # Calculate context representation (weighted sum of encoder states)\n",
    "    context_representation = np.sum(encoder_states * attention_weights[:, np.newaxis], axis=0)\n",
    "    \n",
    "    return {\n",
    "        'attention_scores': attention_scores,\n",
    "        'attention_weights': attention_weights,\n",
    "        'context_representation': context_representation\n",
    "    }\n",
    "\n",
    "# Compute attention\n",
    "result = dot_product_attention(decoder_state, encoder_states)\n",
    "\n",
    "print(\"Attention Scores:\", result['attention_scores'])\n",
    "print(\"Attention Weights:\", result['attention_weights'])\n",
    "print(\"Context Representation:\", result['context_representation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
